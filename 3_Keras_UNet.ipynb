{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, h5py\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave, imshow\n",
    "from skimage.exposure import equalize_adapthist\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_parallel(model, gpu_count):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        start = stride * idx\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    inputs.append(slice_n)                \n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
    "            \n",
    "        return Model(input=model.inputs, output=merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FolderPath = \"../../dataset/ultrasound_nerve_segmentation/\"\n",
    "\n",
    "K.set_image_dim_ordering('tf')\n",
    "#K.set_floatx('float16')\n",
    "\n",
    "#original size : 420x580\n",
    "img_rows=96\n",
    "img_cols=96\n",
    "\n",
    "f_size = 1\n",
    "learning_rate = 1e-2\n",
    "activation = 'elu'\n",
    "\n",
    "smooth = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    with h5py.File(FolderPath + 'X_Train.h5', 'r') as hf:\n",
    "        imgs_train = hf['X_Train'][:]\n",
    "    with h5py.File(FolderPath + 'Y_Train.h5', 'r') as hf:\n",
    "        imgs_mask_train = hf['Y_Train'][:]\n",
    "    \n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "def load_test_data():\n",
    "    with h5py.File(FolderPath + 'X_Test.h5', 'r') as hf:\n",
    "        imgs_test = hf['X_Test'][:]\n",
    "    with h5py.File(FolderPath + 'Y_Test.h5', 'r') as hf:\n",
    "        imgs_mask_test = hf['Y_Test'][:]\n",
    "        \n",
    "    return imgs_test, imgs_mask_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    inputs = Input((img_rows, img_cols, 1))\n",
    "    conv1 = Conv2D(3, (1, 1), activation=activation, padding='same')(inputs)\n",
    "\n",
    "    conv1 = Conv2D(32, (3, 3), activation=activation, padding='same')(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation=activation, padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation=activation, padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation=activation, padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation=activation, padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation=activation, padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(256, (3, 3), activation=activation, padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation=activation, padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Conv2D(512, (3, 3), activation=activation, padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation=activation, padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2,2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=activation, padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=activation, padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    \n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2,2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=activation, padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=activation, padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    \n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2,2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=activation, padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=activation, padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    \n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2,2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=activation, padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=activation, padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    model.summary()\n",
    "    model = make_parallel(model, 2)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exposure_image(X):\n",
    "    Xf = np.array(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        Xf[i] = equalize_adapthist(X[i])\n",
    "    return Xf\n",
    "\n",
    "def preprocess(X,y):\n",
    "    X = (X / 255.).astype(np.float32)\n",
    "    y = (y / 255.).astype(np.float32)\n",
    "\n",
    "    X = exposure_image(X)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_image(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols), dtype=np.float32)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows), preserve_range=True)\n",
    "\n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/.local/lib/python3.5/site-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 96, 96, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 96, 96, 3)     6           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 96, 96, 32)    896         conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 96, 96, 32)    9248        conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 96, 96, 32)    128         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 48, 48, 32)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 48, 48, 64)    18496       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 48, 48, 64)    36928       conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 48, 48, 64)    256         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 24, 24, 64)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 24, 24, 128)   73856       max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 24, 24, 128)   147584      conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 24, 24, 128)   512         conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 12, 12, 128)   0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 12, 12, 256)   295168      max_pooling2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 12, 12, 256)   590080      conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 12, 12, 256)   1024        conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 6, 6, 256)     0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 6, 6, 512)     1180160     max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 6, 6, 512)     2359808     conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 6, 6, 512)     2048        conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTransp (None, 12, 12, 256)   524544      batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 12, 12, 512)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                   batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 12, 12, 256)   1179904     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 12, 12, 256)   590080      conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 12, 12, 256)   1024        conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTransp (None, 24, 24, 128)   131200      batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 24, 24, 256)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                   batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 24, 24, 128)   295040      concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 24, 24, 128)   147584      conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 24, 24, 128)   512         conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTransp (None, 48, 48, 64)    32832       batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 48, 48, 128)   0           conv2d_transpose_3[0][0]         \n",
      "                                                                   batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 48, 48, 64)    73792       concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, 48, 48, 64)    36928       conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 48, 48, 64)    256         conv2d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTransp (None, 96, 96, 32)    8224        batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 96, 96, 64)    0           conv2d_transpose_4[0][0]         \n",
      "                                                                   batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, 96, 96, 32)    18464       concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 96, 96, 32)    9248        conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 96, 96, 32)    128         conv2d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 96, 96, 1)     33          batch_normalization_9[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 7,765,991\n",
      "Trainable params: 7,763,047\n",
      "Non-trainable params: 2,944\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/.local/lib/python3.5/site-packages/ipykernel_launcher.py:44: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/vmadmin/.local/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/vmadmin/.local/lib/python3.5/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model = get_unet()\n",
    "model_checkpoint = ModelCheckpoint('model_UNET.hdf5', monitor='loss', save_best_only=True)\n",
    "model_earlystopping = EarlyStopping(monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_train, imgs_mask_train = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_train, imgs_mask_train = preprocess(imgs_train, imgs_mask_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_train = resize_image(imgs_train)\n",
    "imgs_mask_train = resize_image(imgs_mask_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_train.shape, imgs_mask_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(imgs_train, imgs_mask_train, batch_size=128*2, nb_epoch=20, verbose=1, shuffle=True,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[model_checkpoint, model_earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_test, imgs_id_test = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_test, imgs_id_test = preprocess(imgs_test, imgs_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_test = resize_image(imgs_test)\n",
    "imgs_id_test = resize_image(imgs_id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('model_UNET.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, accu = model.evaluate(imgs_test, imgs_id_test, verbose=1)\n",
    "print(\"loss:{}%, accuracy:{}%\".format(loss*100, accu*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_pred_test = model.predict(imgs_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_dir = 'preds'\n",
    "test_dir = 'tests'\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.mkdir(pred_dir)\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for image_id, image in enumerate(imgs_pred_test):\n",
    "#     image = (image[:,:,0] * 255.).astype(np.uint8)\n",
    "#     imsave(os.path.join(pred_dir, str(image_id) + '_pred.png'), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_compare(idx):\n",
    "    plt.figure(dpi=40)\n",
    "    plt.subplot(131)\n",
    "    imshow(imgs_id_test[idx,:,:,0],cmap='gray')\n",
    "    plt.subplot(132)\n",
    "    imshow(imgs_test[idx,:,:,0],cmap='gray')\n",
    "    imshow(imgs_pred_test[idx,:,:,0],cmap='jet', alpha=0.5)\n",
    "    plt.subplot(133)\n",
    "    imshow(imgs_id_test[idx,:,:,0] - imgs_pred_test[idx,:,:,0], cmap='gray')\n",
    "    plt.savefig(os.path.join(test_dir, 'test' + str(idx) + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ran_idx = np.random.randint(0, imgs_test.shape[0],10)\n",
    "for i,idx in enumerate(ran_idx):\n",
    "    plot_compare(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
